# -*- coding: utf-8 -*-
"""EDA, ML Colab, ML BigQuery, Cohorts, RFM, GigaChat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jaWx7RL34ML_YO-1weUdXK12pnSt0zfk

## EDA, ML Colab, ML BigQuery, Cohorts, RFM, GigaChat

### Импорт библиотек и датасета
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from operator import attrgetter

import plotly.express as px
import plotly.graph_objects as go

import plotly

from google.colab import drive
drive.mount('/content/drive')

import zipfile

with zipfile.ZipFile('/content/drive/MyDrive/Курсовая/eCom events history in Cosmetics shop.zip', 'r') as zip_ref:
    zip_ref.extractall()

df = pd.concat([pd.read_csv('/content/2019-Oct.csv', parse_dates=['event_time']),
                pd.read_csv('/content/2019-Nov.csv', parse_dates=['event_time']),
                pd.read_csv('/content/2019-Dec.csv', parse_dates=['event_time']),
                pd.read_csv('/content/2020-Jan.csv', parse_dates=['event_time'])
                #,pd.read_csv('/content/2020-Feb.csv', parse_dates=['event_time'])
                ],
                ignore_index = True)

df.head()

df.info()

df['product_id'] = df['product_id'].astype('object')
df['category_id'] = df['category_id'].astype('object')
df['user_id'] = df['user_id'].astype('object')

df.isna().sum()

(len(df)-df['category_code'].isna().sum())/len(df)*100

"""Пропуски в **brand, user_session** не критичны, их можно заполнить.
**category_code** при этом практически отсутствует в датасете, поэтому эту колонку можно удалить (данные заполнны на 1.5% )
"""

df = df.drop(['category_code'], axis = 1)
df['brand'] = df['brand'].fillna('no_brand_data')
df['user_session'] = df['user_session'].fillna('user_session')

df.isna().sum()

"""Event types can be:

* view - a user viewed a product
* cart - a user added a product to shopping cart
* remove_from_cart - a user removed a product from shopping cart
* purchase - a user purchased a product
"""

df = df.drop_duplicates(keep='first', inplace=False, ignore_index=False)

df['event_time'] = pd.to_datetime(df['event_time'].dt.date)

len(df)

df.to_csv("/content/dataset_preproc.csv")

"""### Экспорт датасета в BQ для дальнейшей обработки при помощи SQL и хранения предобработанных данных"""

#Dataset with Table name variable
BqDatasetwithtable='TermPaper.ecom_cosmetics_raw_data'
#BQproject name variable
BqProject='guzel-2021-a9e57'
# pandas-gbq method to load data
# append data if data table exists in BQ project
# set chunk size of records to be inserted
df.to_gbq(BqDatasetwithtable, BqProject,chunksize=10000, if_exists='append' )

"""### Импорт датасета из BQ для дальнейшей обработки в Colab"""

from google.cloud import bigquery
!pip install db-dtypes

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

PROJECT_ID = 'guzel-2021-a9e57' # Change to your project.
REGION = 'US'

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery df --project $PROJECT_ID
# 
# SELECT event_type, event_time, product_id, category_id, brand, price, user_id, user_session
# FROM `TermPaper.ecom_cosmetics_raw_data`

df.head()

df.info()

"""### EDA в Colab"""

df.describe()

print(" Минимальная дата", df['event_time'].min(), '\n', "Максимальная дата", df['event_time'].max())

df_summ = df.groupby('event_type').agg({"user_session": "nunique", "price": "sum"}).rename(columns = {'user_session': 'count_events'}).\
            reset_index().sort_values(by=['count_events', 'price'], ascending = False)
df_summ

fig, axes = plt.subplots(1,2)
fig.set_size_inches(21,5)

labels = df_summ['event_type']
colors = sns.color_palette("muted")  #('lightblue', 'pink', 'lightgreen', 'lightgrey', 'mediumseagreen')

#  по кол-ву событий
sizes = df_summ['count_events']

axes[0].pie(sizes, colors = colors, labels = labels, autopct = '%1.1f%%')
patches, text, auto = axes[0].pie(sizes, colors = colors, autopct = '%1.1f%%')
axes[0].axis ('equal')
axes[0].legend (patches, sizes, loc = "lower left")
axes[0].set(title = "Соотношение event_type по количеству событий")

#по цене
sizes1 = df_summ['price'].astype(int)

axes[1].pie(sizes1, colors = colors, labels = labels, autopct = '%1.1f%%')
patches, text, auto = axes[1].pie(sizes1, colors = colors, autopct = '%1.1f%%')
axes[1].axis ('equal')
axes[1].legend (patches, sizes1, loc = "lower left")
axes[1].set(title = "Соотношение суммы товаров по even_type")

"""**Если предположить, что все пользователи сначала смотрят товар, потом добавляют в корзину, а потом покупают, то можно составить конверсионную воорнку** Для этой цели уникальным идентификаором будет связка user_id, user_session. Event_type "remove from cart" убираем из последовательности

"""

'''
funnel = df[df['event_type'].isin(['cart', 'purchase', 'view'])]['event_type'].value_counts().sort_values( ascending = False).reset_index()
for i in range(len(funnel)):
    if i == 0:
        funnel['percents'][i] = 1.00
    else:
        funnel['percents'][i] = (funnel['count'][i]/funnel['count'][i-1]).astype('float64')
funnel
'''

funnel = df[df['event_type'].isin(['cart', 'purchase', 'view'])].groupby('event_type')['user_session'].nunique().sort_values( ascending = False).reset_index()
funnel['prev_count'] = funnel['user_session'].shift(1)
funnel['percentage'] = funnel['user_session']*100/funnel['prev_count']
funnel[['event_type', 'user_session','percentage']]

#funnel.to_csv('/content/drive/MyDrive/Курсовая/funnel.csv')

"""**Сколько сессий приходиться на 1 пользователя: всего и завершившихся покупкой**"""

print("Сессий на 1 пользователя: ", df['user_session'].nunique()/df['user_id'].nunique(), '\n',
      "Сессий с покупками на 1 пользователя: ", df[df['event_type']=="purchase"]['user_session'].nunique()/df[df['event_type']=="purchase"]['user_id'].nunique())

'''
for i in range(len(df_test)):
    if i ==0:
        df_test['test'][i] = 100
    else:
        df_test['test'][i] = df_test['price'][i]/df_test['price'][i-1]
df_test
'''

((df['brand'].value_counts().nlargest(16))/1000).reset_index().rename(columns = {"count":"count, K"})

#((df['brand'].value_counts().nlargest(16))/1000).reset_index().rename(columns = {"count":"count, K"}).to_csv('/content/drive/MyDrive/Курсовая/brands.csv')

((df['brand'].value_counts().nlargest(16))/1000).reset_index()[1::].plot(kind="bar", title="brand counts, K")

#df[df['product_id']==5773203]['price'].sum()/df[df['product_id']==5773203]['price'].count()

df[['product_id', 'price']].groupby(['product_id']).agg({"product_id":"count", "price":"mean"}).plot.scatter(x='product_id', y = 'price', c = "darkblue")

df['month_year'] = df['event_time'].dt.to_period('M')
month_summary = df.groupby(['month_year']).agg({"price":"sum", "product_id":"count", "user_id":"nunique"})
month_summary['product_per_user'] = month_summary['product_id']/month_summary['user_id']
month_summary['purchase_per_user, curr'] = month_summary['price']/month_summary['user_id']
month_summary.rename(columns = {"price": "Sales, currency", "product_id":"Sales, units", "user_id":"Uniq.Users"})

#month_summary.to_csv('/content/drive/MyDrive/Курсовая/month_summary.csv')

print(f"Кол-во уникальных пользователей {df['user_id'].nunique()}")

df['date'] = df['event_time'].dt.round('D') #dt.date
daily_summary = df[df['event_type']=="purchase"].groupby(['date']).agg({"price":"sum", "product_id":"count", "user_id":"nunique"})
daily_summary['product_per_user'] = daily_summary['product_id']/daily_summary['user_id']
daily_summary['sales_per_user'] = daily_summary['price']/daily_summary['user_id']

plt.figure(figsize=(10,7))
plt.plot('date', 'product_per_user', data = daily_summary.reset_index()[['date', 'product_per_user']], c = "darkblue", label ="Units per user")
plt.plot('date', 'sales_per_user', data = daily_summary.reset_index()[['date', 'sales_per_user']], c = "forestgreen", label = "Sales per user, currency")
plt.title("Sales (currency, units) per user")
plt.legend(loc = 'upper left')
plt.show()

plt.figure(figsize=(15,7))
plt.plot('date', 'user_id', data = df.groupby('date')['user_id'].nunique().reset_index(), c = "darkblue", label ="Unique users total")
plt.plot('date', 'user_id', data = df[df['event_type']=="purchase"].groupby('date')['user_id'].nunique().reset_index(), c = "green", label ="Unique users w purchases")
plt.title("Unique users")
plt.legend(loc = 'upper left')
plt.show()

plt.figure(figsize=(15,7))
plt.plot('date', 'user_id', data = df[df['event_type']=="purchase"].groupby('date')['user_id'].nunique().reset_index(), c = "green", label ="Unique users w purchases")
plt.title("Unique users")
plt.legend(loc = 'upper left')
plt.show()

#df[df['event_type']=="purchase"].groupby('date')['user_id'].nunique().reset_index().to_csv('/content/drive/MyDrive/Курсовая/unique_users_w_purchases.csv')

df['day_of_week'] = df['date'].dt.dayofweek + 1

dayofweek_names = dict({1: "Monday",
                         2: "Tuesday",
                         3: "Wednesday" ,
                         4: "Thursday",
                         5: "Friday",
                         6: "Saturday",
                         7: "Sunday"})

df['Day_names'] = df['day_of_week'].map(dayofweek_names)
df = df.sort_values(by = 'date', ascending = True)

#fig, axes = plt.subplots(2,2)
#fig.set_size_inches(21,5)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(21, 5))

for idx,i in enumerate(list(df['month_year'].unique())):
    df[df['month_year']==i].groupby(['Day_names', 'day_of_week'])['user_id'].nunique().reset_index().sort_values(by = 'day_of_week').plot(
                                                                                        'Day_names', 'user_id',
                                                                                         subplots=True,
                                                                                         kind="line",
                                                                                         title = (f"Unique users by days of week"),
                                                                                         color = "forestgreen",
                                                                                         label = f"Unique users by days of week in {i}",
                                                                                         ax=axes[idx // 2, idx % 2] )

#df.groupby(['month_year','Day_names', 'day_of_week'])['user_id'].nunique().reset_index().sort_values(by = 'day_of_week').to_csv('/content/drive/MyDrive/Курсовая/unique_users_by_days_of_week.csv')

#df[df['event_type']=="purchase"].groupby(['month_year','Day_names', 'day_of_week'])['user_id'].nunique().reset_index().sort_values(by = 'day_of_week').to_csv('/content/drive/MyDrive/Курсовая/unique_users_by_days_of_week_w_purchase.csv')

"""### Когортный, RFM в Colab

#### Когортный
Разобью на когорты по неделе первой покупки. Самую ранню дату в датасете будем считать датой первой покупки. Рассматриваю 2019 год.
"""

'''
df['week_num'] = df['date'].dt.year.astype('str') +"-"+ df['date'].dt.isocalendar().week.astype('str')
df['cohort'] = df.groupby('user_id')['date'].transform('min').dt.year.astype('str') +"-"+ df.groupby('user_id')['date'].transform('min').dt.isocalendar().week.astype('str')
df['cohort']
'''

# df загрузить из BQ
purchased = df[(df['event_type']=="purchase") & (df['price']>0)]

purchased['date'] = pd.to_datetime(purchased['event_time'].dt.date)

purchased.head()

purchased['week_num'] = purchased['date'].dt.isocalendar().week
purchased['cohort'] = purchased.groupby('user_id')['date'].transform('min').dt.isocalendar().week
purchased['cohort_year'] = purchased.groupby('user_id')['date'].transform('min').dt.year

purchased['year'] = purchased['date'].dt.year
purchased = purchased[purchased['year'] == 2019]
purchased['week_num'] = np.where((purchased['week_num'] == 1), 52, purchased['week_num'])
purchased['cohort'] = np.where(((purchased['cohort'] == 1) & (purchased['cohort_year'] == 2019)), 52, purchased['cohort'])

#проверка
#purchased[purchased['cohort'] =='2019-52']['date'].min()
purchased[purchased['month_year'] == '2019-12'].sample(5)

# проверка
purchased[(purchased['cohort'] == 52) & (purchased['date'] == '2019-12-30')].sample(5)

purchased.groupby('week_num')['user_id'].nunique()

purchased[purchased['week_num']==4]

purchased['period_num'] = (purchased.week_num - purchased.cohort)   #.apply(attrgetter('n'))
df_cohort = purchased[purchased['date']<='2019-12-31'].groupby(['cohort', 'period_num']).agg(n_accounts = ('user_id', 'nunique')).reset_index(drop= False)

cohort_pivot = df_cohort.pivot_table(index = 'cohort',
                                     columns = 'period_num',
                                     values = 'n_accounts')

plt.figure(figsize = (16,5))
sns.heatmap(cohort_pivot, annot = True, fmt = 'g',
            cmap = sns.color_palette("ch:s=.25, rot=-.25", as_cmap = True))
plt.show()

"""#### Ретеншн"""

cohort_size = cohort_pivot.iloc[:,0]
retention_matrix = cohort_pivot.divide(cohort_size, axis = 0)
plt.figure(figsize=(16,5))
sns.heatmap (retention_matrix, fmt = '.0%', annot = True,
             cmap = (sns.color_palette("light:#5A9", as_cmap = True)) )
plt.show()

cohort_size.describe()

"""#### RFM для клиентов"""

rfm_raw = df[(df['price']>0) & (df['event_type']=="purchase")]

rfm_raw['category_id'].nunique()

purchased.head()

purchased['year'] = purchased['date'].dt.year
purchased['month'] = purchased['date'].dt.month
purchased = purchased.drop('event_time', axis = 1)

"""**Revenue = Monetary**

"""

rfm_m = pd.DataFrame(purchased.groupby([purchased.year, purchased.month])['price'].sum()).rename(columns={"price":"Revenue"})

rfm_m['MonthlyGrowth'] = rfm_m['Revenue'].pct_change()
rfm_m.head()

rfm_m['ActiveCustomers'] = pd.DataFrame(purchased.groupby([purchased.year,purchased.month])['user_id'].nunique())
rfm_m

rfm_m['MonthlyOrderCount'] = pd.DataFrame(purchased.groupby([purchased.year,purchased.month])['user_session'].nunique())
rfm_m

purchased1 = purchased.groupby(['year','month','user_session'])['price'].sum().reset_index()
rfm_m['MonthlyOrderAverage'] =  pd.DataFrame(purchased1.groupby([purchased1.year,purchased1.month])['price'].mean())
rfm_m

# показываем индекс, чтобы было видно месяц и год
rfm_m.index.set_names(['Year', 'Month'], inplace = True)
rfm_m

rfm_m.index

"""**Делаем дата-фрейм с первой покупкой покупателя**"""

customer_fist_purchase = purchased.groupby('user_id')['date'].min().reset_index()

customer_fist_purchase.columns = ['user_id','FirstPurchaseDate']

customer_fist_purchase['FirstPurchaseYearMonth'] = customer_fist_purchase['FirstPurchaseDate'].map(lambda date: 100*date.year+date.month)
customer_fist_purchase

#Складываем две таблицы - с первой покупкой и основную
purchased_1stpurch = pd.merge(purchased, customer_fist_purchase, on='user_id')
#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html

purchased_1stpurch.head()

#Создаем новую колонку - пока по дефолту все пользователи - New
purchased_1stpurch['UserType'] = 'New'
#Создаем колонку ['YearMonth'] в нужном формате
purchased_1stpurch['YearMonth'] = purchased_1stpurch['date'].map(lambda date: 100*date.year + date.month)

#Теперь мы можем добавить пользователей с другим статусом!
purchased_1stpurch.loc[purchased_1stpurch['YearMonth'] > purchased_1stpurch['FirstPurchaseYearMonth'],'UserType'] = 'Existing'

#Считаем Revenue для каждого типа пользователя
customer_kpis = pd.DataFrame(purchased_1stpurch.groupby([purchased_1stpurch.year,purchased_1stpurch.month, purchased_1stpurch.UserType])['price'].sum())
customer_kpis.index.set_names(['Year', 'Month', 'UserType'], inplace = True)
customer_kpis

#Сравниваем новых и вернувшихся
customer_kpis.unstack().plot.bar(y='price', title='New vs Existing Customer Monthly Revenue', rot=0);

#Создаем новый дата-фрейм основываясь на user_id
customer = pd.DataFrame(purchased['user_id'].unique())
customer.columns = ['user_id']

#Последняя дата сессии date.max()
recency = purchased_1stpurch.groupby('user_id').date.max().reset_index()
recency.columns = ['user_id','LastPurchaseDate']

# Максимальное значение (самая свежая покупка)
LastInvoiceDate = recency['LastPurchaseDate'].max()

# Новизна покупки для каждого покупателя по сравнениею с LastInvoiceDate
recency['Recency'] = (LastInvoiceDate - recency['LastPurchaseDate']).dt.days
customer = pd.merge(customer, recency[['user_id','Recency']], on='user_id')

# Считаем количество сессий на каждого покупателя
frequency = purchased_1stpurch.groupby('user_id').user_session.count().reset_index()
frequency.columns = ['user_id','Frequency']
customer = pd.merge(customer, frequency, on='user_id')

# Группируем выручку по покупателю
revenue =  purchased_1stpurch.groupby('user_id').price.sum().reset_index()
revenue.columns = ['user_id', 'Monetary']
customer = pd.merge(customer, revenue, on='user_id')

customer.head()

recency.describe()

customer.sort_values('Monetary', ascending = False) #сортируем
customer["Monetary"] = customer["Monetary"].astype(int) #приводим к int виду

"""**Разделим датасет на 5 частей**"""

customer['Recency_group'] = pd.qcut(customer.Recency.sort_values(ascending = False), q=5, labels = ['1','2','3','4','5'][::-1])
customer['Frequency_group'] = pd.qcut(customer.Frequency.rank(method='first'), q=5, labels = ['1','2','3','4','5'], duplicates = 'raise')                              #, ordered = True)
customer['Monetary_group'] = pd.qcut(customer.Monetary.sort_values(ascending = True), q=5, labels = ['1','2','3','4','5'])                                             #, ordered = True)
customer['RFM'] = customer['Recency_group'].astype(str) + customer['Frequency_group'].astype(str) + customer['Monetary_group'].astype(str)
customer.head()

"""### Нейросеть для пояснений по RFM (Giga chat)"""

from google.colab import userdata
auth = userdata.get('auth')

import requests
import uuid
import json

#import requests
#import uuid

def get_token (auth_token, scope = 'GIGACHAT_API_PERS'):
    rq_uid = str(uuid.uuid4())
    #API url:
    url = "https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
    # Заголовки
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept': 'application/json',
        'RqUID': rq_uid,
        'Authorization': f'Basic {auth_token}'
    }
    payload = {
        'scope': scope
    }

    try:
        response = requests.post(url, headers = headers, data = payload, verify = False) # False т.к. Минцифры нет
        return response
    except requests.RequestException as e:
        print (f'Ошибка: {str(e)}')
        return -1

# получение токена доступа

response = get_token(auth)
if response !=1:
      print(response.text)
      giga_token = response.json()['access_token']


# Получение перечня моделей
# url с сайта gigachat для получения перечня моделей

url = "https://gigachat.devices.sberbank.ru/api/v1/models"

payload = {}
headers = {
      'Accept': 'application/json',
      'Authorization': f'Bearer {giga_token}'
}
response = requests.request("GET", url, headers=headers, data=payload, verify = False)
#print (response.text)


def get_chat_completion (auth_token, user_message):
    url = 'https://gigachat.devices.sberbank.ru/api/v1/chat/completions'
    payload = json.dumps ({
        "model": "GigaChat-Max",
        "messages": [
            {
              "role": "user",
              "content": user_message
            }
          ],
          "temperature": 1,
          "top_p": 0.1,
          "n": 1,
          "stream": False,
          "max_tokens": 512,
          "repetition_penalty": 1,
          "update_interval": 0
    })
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        'Authorization': f'Bearer {auth_token}'
    }
    try:
          response = requests.request("POST", url, headers = headers, data = payload, verify = False) # False т.к. Минцифры нет
          return response
    except requests.RequestException as e:
        print (f'Ошибка: {str(e)}')
        return -1

import warnings
warnings.filterwarnings('ignore')

while True:
    req = input("Введите запрос (Пример запроса по RFM: RFM данные разбиты на 5 групп. Что означает группа 555?) ")
    print("----------------------------------------------------------------------------------------------------------------------")
    answer = get_chat_completion(giga_token, req).json()['choices'][0]['message']['content']
    print(answer)
    print("-----------------------------------------------------------------------------------------------------------------------")
    req2 = input("Хотите что-то еще уточнить? Да / Нет ")
    if str.lower(req2) == "да":
        req
    else:
        break

"""### ML  BQ"""

# Commented out IPython magic to ensure Python compatibility.
# # сделаю представление сгруппированных для ML данных. Буду рогнозировать кол-ва или суммы покупок в грануляции дней.
# # прогноз будет на январь 2020.
# 
# %%bigquery --project $PROJECT_ID
# 
# CREATE OR REPLACE VIEW `guzel-2021-a9e57.TermPaper.ecom_cosmetics_grouped_ML` as (
# SELECT
#   date_trunc(`event_time`, day) as Date,
#   count(`category_id`) as Category_qty,
#   sum(`price`) as Sales,
#   count(`product_id`) as Products_qty,
#   count(distinct `user_id`) as Customers,
#   count(distinct `user_session`) as Sessions
# FROM
#   `guzel-2021-a9e57.TermPaper.ecom_cosmetics_raw_data`
#   where
#   event_type = 'purchase'
#   and `price` > 0
#   group by 1);

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery df_ml  --project $PROJECT_ID
# 
# SELECT
#   Date,
#   Category_qty,
#   Sales,
#   Products_qty,
#   Customers,
#   Sessions
# FROM
#   `guzel-2021-a9e57.TermPaper.ecom_cosmetics_grouped_ML`;

df_ml.head()

# даты начала и окончания для обучающих данных
ARIMA_PARAMS = {
    'TRAININGDATA_STARTDATE': '2019-10-01',
    'TRAININGDATA_ENDDATE': '2019-12-31',
}
ARIMA_PARAMS

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery df_ml_train --params $ARIMA_PARAMS  --project $PROJECT_ID
# 
# CREATE OR REPLACE TABLE TermPaper.training_data AS (
#     SELECT
#         Date,
#         Customers
#     FROM
#         `guzel-2021-a9e57.TermPaper.ecom_cosmetics_grouped_ML`
#     GROUP BY
#         1, 2
#     HAVING
#         Date BETWEEN @TRAININGDATA_STARTDATE AND @TRAININGDATA_ENDDATE
#     );
# 
# SELECT
#      Date,
#     Customers
# FROM
#      TermPaper.training_data
# ORDER BY 1, 2

df_ml_train.info()

df_ml_train.head()

# Commented out IPython magic to ensure Python compatibility.
# #создание модели
# 
# %%bigquery --project $PROJECT_ID
# 
# CREATE OR REPLACE MODEL TermPaper.arima_model
# 
# OPTIONS(
#   MODEL_TYPE='ARIMA_PLUS',
#   TIME_SERIES_TIMESTAMP_COL='Date',
#   TIME_SERIES_DATA_COL='Customers',
#   HOLIDAY_REGION='US'
# ) AS
# 
# SELECT
#     Date,
#     Customers
# FROM
#    TermPaper.training_data

# Commented out IPython magic to ensure Python compatibility.
# # оценка модели
# %%bigquery --project $PROJECT_ID
# 
# SELECT
#   *
# FROM
#   ML.EVALUATE(MODEL TermPaper.arima_model)

# Commented out IPython magic to ensure Python compatibility.
# # прогнозирование
# 
# %%bigquery df_ml_forecast --project $PROJECT_ID
# 
# DECLARE HORIZON STRING DEFAULT "30"; #number of values to forecast
# DECLARE CONFIDENCE_LEVEL STRING DEFAULT "0.95";
# # Выполняет динамическую инструкцию SQL на лету
# EXECUTE IMMEDIATE format("""
#     SELECT
#       *
#     FROM
#        ML.FORECAST(MODEL TermPaper.arima_model,
#                   STRUCT(%s AS horizon,
#                          %s AS confidence_level)
#                  )
#     """,HORIZON,CONFIDENCE_LEVEL)

df_ml_forecast.head()

px.line(df_ml_forecast.sort_values(by=['forecast_timestamp'], ascending=[True]), x='forecast_timestamp', y='forecast_value')

# Commented out IPython magic to ensure Python compatibility.
# # коэффициенты модели
# %%bigquery --project $PROJECT_ID
# 
# SELECT
#   *
# FROM
#    ML.ARIMA_COEFFICIENTS(MODEL TS.arima_model)

df_ml_forecast.info()

# Commented out IPython magic to ensure Python compatibility.
# %%bigquery df_ml_actual --params $ARIMA_PARAMS --project $PROJECT_ID
# 
# DECLARE HORIZON STRING DEFAULT "29"; #number of values to forecast
# 
# SELECT
#     Date,
#     Customers
# FROM
#     `guzel-2021-a9e57.TermPaper.ecom_cosmetics_grouped_ML`
# GROUP BY
#     1, 2
# HAVING
#     Date BETWEEN '2020-01-01' and '2020-01-29'
# ORDER BY
#    1

df_ml_forecast[['forecast_timestamp',	'forecast_value']].rename(columns = {"forecast_timestamp": "Date"})

plt.figure(figsize=(10,7))
plt.plot('Date', 'Customers', data = df_ml_actual, color = "green", label = 'fact')
plt.plot('forecast_timestamp', 'forecast_value', data = df_ml_forecast, color = "darkblue", label = "forecast")
plt.legend(loc = 'best')
plt.show()

(df_ml_actual['Date']+pd.Timedelta(30, "d")).max(), df_ml_actual['Date'].max()

"""### ML в Colab"""

# сначала загрузить датасет из BQ еще раз
df.info()

df_ml_colab = df.copy(deep=True)

df.head()

df_ml_colab['date'] = pd.to_datetime(df_ml_colab['event_time'].dt.date)

#df_ml_colab['event_time'].dt.round('d') возвращает с часовым поясом (этомешает при ML)

df_ml_colab.info()

df_ml_colab.head()

df_ml_colab.isna().sum()

"""##### Подготовка данных
- Группировка по дате. Прогоноз так же как в BQ будет по количеству покупателей
"""

df_ml_colab = df_ml_colab.drop('event_time', axis = 1)

#df_ml_colab = df_ml_colab[(df_ml_colab['event_type'] == "purchase") & (df_ml_colab['price']>0) & (df_ml_colab['date']<'2020-01-01')][['date', 'user_id']].groupby('date')['user_id'].nunique().reset_index()
df_ml_colab = df_ml_colab[(df_ml_colab['event_type'] == "purchase") & (df_ml_colab['price']>0)][['date', 'user_id']].groupby('date')['user_id'].nunique().reset_index()
df_ml_colab.columns = ['date','customers']
df_ml_colab.head()

df_ml_colab.boxplot('customers')

df_ml_colab.plot(kind = "line", x = 'date', y = 'customers', figsize = (15,7))

Q1 = df_ml_colab['customers'].quantile(0.25)
Q3 = df_ml_colab['customers'].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

df_ml_colab['month'] = df_ml_colab['date'].dt.month
df_ml_colab['customers_new'] = np.where((df_ml_colab['customers'] < (Q1 - 1.5 * IQR)) | (df_ml_colab['customers'] > (Q3 + 1.5 * IQR)),
                       df_ml_colab.groupby('month')['customers'].transform("median"), df_ml_colab['customers'])
df_ml_colab.info()

df_ml_colab.boxplot('customers_new')

plt.figure(figsize = (15,7))
plt.plot('date', 'customers_new', data = df_ml_colab, color = "red", label = "сглаженные")
plt.plot('date', 'customers', data = df_ml_colab, color = "lightblue", label = "изначальные")
plt.title("После сглаживания выбросов")
plt.legend(loc='best')
plt.show()

df_ml_colab = df_ml_colab.drop(['month', 'customers'], axis = 1).rename(columns = {"customers_new":"customers"})
df_ml_colab.head()

"""#### Darts"""

!pip install darts

#Loading the package
from darts import TimeSeries
from darts.models import ExponentialSmoothing

# Create a TimeSeries, specifying the time and value columns
series = TimeSeries.from_dataframe(df_ml_colab, 'date', 'customers')

# Set aside the last 30 days as a test series to be used for predictions
train, val = series[:-30], series[-30:]

model = ExponentialSmoothing()
model.fit(train)
prediction = model.predict(len(val), num_samples=1000)

series.plot()
prediction.plot(label='forecast', low_quantile=0.05, high_quantile=0.95)
plt.legend()

"""#### GreyKite

"""

!pip install greykite

from greykite.framework.templates.autogen.forecast_config import ForecastConfig
from greykite.framework.templates.autogen.forecast_config import MetadataParam
from greykite.framework.templates.forecaster import Forecaster
from greykite.framework.templates.model_templates import ModelTemplateEnum
from greykite.framework.utils.result_summary import summarize_grid_search_results

# Specifies dataset information
metadata = MetadataParam(
     time_col="date",  # name of the time column
     value_col="customers"  # name of the value column
     #freq= "MS" for Montly at start date
     #train_end_date='2019-12-01'
 )
forecaster = Forecaster()
result = forecaster.run_forecast_config(
     df=df_ml_colab,
     config=ForecastConfig(
         model_template=ModelTemplateEnum.SILVERKITE.name,
         forecast_horizon=30,  # forecasts 100 steps ahead
         coverage=0.95,  # 95% prediction intervals
         metadata_param=metadata
    )
)

ts = result.timeseries
fig = ts.plot()
plotly.io.show(fig)

backtest = result.backtest
fig = backtest.plot()
plotly.io.show(fig)

fig = backtest.plot_components()
plotly.io.show(fig)